\chapter{Machine learning}
%\section{What is machine learning?}

The Scientific Revolution during the Renaissance \marginpar{A historical note}
was fuelled by the realisation that Nature can be parametrised using only a handful of equations.
With these tools, and sufficient measurements, natural philosophers could use the universal equations to fully model the mechanical universe.
As mathematical tools improved, increasingly more complex systems could be analysed, such as orbits of astrophysical objects or complex optical designs.
Furthermore, the development of computers allowed for an explosion in the size and breadth of tractable problems, such as dynamics of atomic nuclei, or weather forecast.

These applications were either developed from first principles, or as effective theories; but contained at their core, relatively simple mathematical models.
For example, modelling the dynamics of $n$-bodies gravitationally interacting are very rich and complicated to solve, but they all spawn from two simple equations: $\vec{F} = m \cdot \vec{a}$  and $F_g=G\frac{m_1  m_2}{r^2}$.

But the advent of computers not only gave us mathematical muscle, it provided us with large amounts of data on the real world.
Data that can be used to fit statistical models, even in the absence of underlying mathematical theories, such as recognising objects in images, identifying where in the cell a protein is going to end up, or translating natural language.
The Scientific Revolution of the 16th century brought the concept \emph{``if it can be measured, it can be modelled"}, but the Data Revolution of the 20th century expanded it \emph{``if it can be \emph{represented}, it can be modelled"}.
We went from measuring the variables present in our equations, to representing the world in numbers in a computer.

%This opens new oportunities to tackle and explore anything we 

Machine learning \marginpar{What is machine learning?}
is the study of statistical models that can create inferences from collections of examples. 
The models can be as concrete as relating the voltage and measured current intensity in a circuit, as complex as relating the sensory input of a rocket with its control, or as abstract as mapping natural images to the text describing its contents.

Instead of designing algorithms that simulate reality from first principles, that can be insurmountably complex and time-consuming; a machine learning model uses data: the algorithm depends on a series of free parameters that are deduced from the examples.
\marginpar{The costs}
This flexibility comes with a price: in the programmer has relinquished the control to the dataset, and sacrificed the possibility of a mechanistic interpretation.
More on this on Section~\ref{sec:wrong}.

\section{Classification and typology}
Machine learning tasks can be classified according to several criteria.
Here are, in broad strokes, some of the main types that cover the majority of the machine learning problems according to different criteria.

\subsection{Do we have labels?}
\begin{itemize}
\item \emph{Supervised:} our training data has assigned labels, and we want to predict them to new data. \emph{Ex: protein secondary structure prediction, linear regression.}
\item \emph{Unsupervised:} we do not have data with annotated target values. \emph{Ex: clustering, dimensionality reduction.}
\end{itemize}

The focus of this thesis will be on supervised tasks.

\subsection{Are labels categorical or continuous?}
The supervised tasks can be again divided depending on the nature of the labels:
\begin{itemize}
\item \emph{Classification:} our labels are categorical variables. \emph{Ex: image recognition, automated transcription of speech, presence or absence of tumours}
\item \emph{Regression:} we are interested in the value of continuous variables. \emph{Ex: curve fitting, counting.}
\end{itemize}

\section{The machine learning spectrum}
We can design machine learning models with different degrees of restrictions, or parametric assumptions.
A more restricted model needs less data to converge, and its performance will not be hindered if the underlying assumptions are correct.
On the other hand, if these restrictions are not accurate, the model will be biased and its performance, limited.

If we instead relax the parametric assumptions we obtain a more flexible model, capable of tackling more complex problems.
But this versatility comes with a cost: they require more data to train.

\begin{center}
	\missingfigure[figcolor=white]{Spectrum diagram}
\end{center}


Can we take it to the extreme?
\marginpar{A theoretical result} 
Can we train a model completely free  of assumptions in the case of infinite data? The No Free Lunch Theorem \citep{no_free_lunch} says, averaging over all problems, all algorithms are equally good.
In other words, without inputting domain knowledge, we cannot do better than random.

\section[Traditional machine learning]{A point of comparison: traditional machine learning}
In this section, I will give an overview to some of the most popular supervised machine learning algorithms to illustrate how they work, and the kind of underlying assumptions they operate under.
I will describe them in their simplest form, be it for regression or classification, but both can be easily generalised: a regression algorithm can be turned to a binary classifier mapping the positive and negative labels to $(1, -1)$ or vice-versa.
A binary classifier can be used in a problem with $N$ classes by either training $N$ binary classifiers for each class versus the rest, or all the $\binom{N}{2}$ pairwise binary classifications.

\subsection{Ordinary Least Squares, Ridge, and LASSO}
\subsection{Multi Layer Perceptron}
\subsection{Support Vector Machines}
A Support Vector Machine, or SVM, is in its simplest form, a supervised, binary classification algorithm that tries to find the hyperplane that maximises the separation of the two groups.
To account for noise, we can include a slack parameter, that will ignore points that are too close to the boundary.
Figure~\ref{fig:svm} illustrates an example of classifying two species from the classic Iris dataset~\cite{iris_dataset}.


\begin{figure}
\centering
\subcaptionbox{Linear kernel}{\includegraphics[width=0.45\textwidth]{machine_learning/figures/svm.pdf}}%
\hfill
\subcaptionbox{RBF kernel\label{subfig:svm_rbf}}{\includegraphics[width=0.45\textwidth]{machine_learning/figures/svm_rbf.pdf}}%
\caption{Classification of two species of Iris flowers according to the length and width of sepals.}\label{fig:svm}
\end{figure}

It can be generalised \marginpar{The kernel trick} to non-flat boundaries using the so-called \emph{kernel trick}, where Euclidean distances are replaced with an arbitrary measure of similarity given by positive-definite kernel function:
$$||\vec{x}_1 - \vec{x}_2|| \rightarrow k(\vec{x}_1, \vec{x}_2)$$

For example, in Figure~\ref{subfig:svm_rbf} we have used a Radial Basis Function:

$$k(\vec{x}_1, \vec{x}_2) = e^{-\gamma ||\vec{x}_1 - \vec{x}_2||^2 },$$
which implicitly projects the data into an infinite-dimensional space.

Another way of interpreting the kernel trick is to think of it as learning a deformation of the space that makes the data separable by a hyperplane, a linear function.

\subsection{$k$-Nearest Neighbour}
A Nearest Neighbour classifier takes the $k$ closest points in the training set, and predicts the most common label.
The choice of $k$ is a balance between noise and flexibility: smaller values give more distinct frontiers, but are more susceptible to noise.


\begin{figure}[htb]
	\centering
	\subcaptionbox{Closest neighbour}{\includegraphics[width=0.45\textwidth]{machine_learning/figures/knn_1}}
	\hfill
	\subcaptionbox{Average of the five closest neighbours.}{\includegraphics[width=0.45\textwidth]{machine_learning/figures/knn_5}}
	\caption{Classification of three species of Iris flowers according to the length and width of sepals.}\label{fig:knn}
\end{figure}
\subsection{Decision tree}
\subsection{Random Forest}
\subsection{Gaussian Processes}
A Gaussian Process takes as an input a set of data points $(x, y)$ that are assumed to be generated by a latent, unknown function $f(x)$ that we wish to infer, plus Gaussian noise. The output is a \emph{probability distribution} over functions, that should be interpreted as the likelihood for each given function to have produced the observed data.


\begin{figure}[htb]
	\centering
	\subcaptionbox{Complete data}{\includegraphics[width=0.45\textwidth]{machine_learning/figures/sin_toy}}
	\hfill
	\subcaptionbox{Gapped data.}{\includegraphics[width=0.45\textwidth]{machine_learning/figures/sin_toy_gapped}}
	\caption{Reconstruction of the $\sin$ function using Gaussian Processes.
	Note the larger uncertainty when there is a gap in the training set.}\label{fig:gp_toy}
\end{figure}


Consider a vector space over functions\footnote{This is a mathematical justification. If you are not interested in Hilbert spaces, jump to the last equation.}, called Hilbert space $\mathscr{H}$, and define a complete, orthonormal basis ($\{\phi_i(x)\}_{i=0}^{n}$).
Any function $f$ in this space can be decomposed:

 $$f(x) = \sum_{i=0}^{n} c_i \phi_i(x)$$
 
In GP, we define our Hilbert space through a positive semidefinite \emph{covariance function} $k(x, x')$
This induces a metric defined through the distance:
	
	$$	d(f, g) =\int_{\mathcal{R}}  f(x) k(x, x') g(x') dx dx' $$,
	
and a series of eigenfunctions of the covariance function.
	
For example,\marginpar{RBF appears again} the previously seen radial basis covariance function defines the Hilbert space of $C^\infty \cap L^2$ functions: all the smooth functions of squared-integrable..
	
The GP will thus project our $n$ data points into this $N$ dimensional space (in general, $N >> n$; usually $N = \infty$). Note that, due to geometry, our $n$ data points must live in a $n$-dimensional subspace of $\mathscr{H}$.
The output is an estimation $\hat f$ of our latent function $f$, that can be interpreted as a decomposition in the eigenfunctions of our covariance function.
	
	$$\hat f(x) = \sum_{i=0}^{n} c_i \phi_i(x)$$
	
But, unlike other procedures, $c_i$ have probability distributions over them.
We can use them to sample likely candidates for the latent function, as shown in the Figure~\ref{fig:gp_sampling}
	
\begin{figure}[hbt]
\centering
	\includegraphics[width=0.4\textwidth]{machine_learning/figures/sin_samples}
	\caption{Posterior samples from Gaussian Processes.}\label{fig:gp_sampling}
\end{figure}

\subsection{Isotonic regression}
Isotonic regression minimises the squared errors of a function that is piecewise constant, and non decreasing.
Given enough data points, it can fit arbitrarily complex curves, as long as they are monotonous.
This method is particularly useful in calibration to turn predicted scores into actual probabilities.
\todo{figure}
\section{On the wrongness of machine learning}\label{sec:wrong}
