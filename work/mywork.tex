\chapter{Paper contributions}

\section*{Paper \textcolor[cmyk]{0, 0.87, 0.68, 0.32}{I}}
\begin{center}
	\textsc{ProQ3D: improved model quality assessments using deep learning.}
	\emph{Bioinformatics}, 2017.
\end{center}
\noindent
ProQ3 \citep{ProQ3} was a \MQA{} program developed in our lab that combines a large number of input features -- such as atom and residue contacts, secondary structure predictors, and Rosetta energy terms -- with an \SVM.
In this paper, we took the same features and dataset used to develop ProQ3, but replaced the machine learning algorithm with a multi-layer perceptron.
This allows us to make full use of the whole dataset, and boosted its performance, improving the correlation between predicted and true from 0.85 to 0.90.

The main limitation of ProQ3D from a theoretical point of view is that the features are engineered for a traditional machine learning framework, as opposed to deep learning.
That is, many features and derivations of features, with no attempt to preserve the structure.
From a practical consideration, ProQ3D requires several predictors and other programs, that while they are all freely available for researchers, we cannot distribute as a single package. Hence, installing it and running is slow and difficult.

On the other hand, ProQ3 is the product of several generations of progressive improvements in model quality assessment, and ProQ3D keeps all of them, with a minimal, focalised change. 

\section*{Paper  \textcolor[cmyk]{0, 0.87, 0.68, 0.32}{II}}
\begin{center}
	\textsc{Large-scale structure prediction by improved contact predictions and model quality assessment.}
	\emph{Bioinformatics}, 2017.
\end{center}

\noindent
Here we present a pipeline for contact-based \emph{ab-initio} protein structure prediction.
It predicts contacts with PconsC3, folds models with \CONFOLD, and selects and evaluates the accuracy using Pcons and ProQ3.
This work combines, in a single package, the programs for structure prediction developed in the group.

We then applied to all the \textsc{pfam} families, of both known and unknown structure, and benchmarked our quality estimation methods.
We found out that a linear combination of the uniformity of the different models -- computed with Pcons \citep{pcons} -- , the ProQ3 predictions, and the agreement with predicted contacts yielded the best results.

The better the contact map, the better the model; and the main driving factor is the quality and depth of the \MSA, with some exceptions.
We can successfully model ($TM>0.5$, 0.1 \textsc{fpr}) 558 \textsc{pfam} families.
\citet{ovchinnikov2017protein} published a similar study based on metagenomics data.
They are successful in a comparable number of families, but only 26\% of them coincide.
This shows that independent methods are still useful, as they can succeed where others fail.


\section*{Paper \textcolor[cmyk]{0, 0.87, 0.68, 0.32}{III}}
\begin{center}
	\textsc{PconsC4: fast, accurate and hassle-free contact predictions.}
	\emph{Bioinformatics}, 2018.
\end{center}

\noindent
\citet{ultra_deep_contacts} were the first group to bring the full force of deep learning to contact prediction, and have been consistently ranked amongst the top performing groups in \CASP.
Unfortunately, their method is only available as a web server, and not for download.
Other programs, such as PconsC3~\citet{PconsC3}, developed in this lab, have a large number of dependencies, making it hard to install, and slow to run.

PconsC4 is a contact predictor designed to be fast and easy to use, clocking an average of 12 s per \MSA.
This is more than twice as fast as the reference implementation of Gauss\DCA{} \citep{GaussDCA}, and 244 times faster than PconsC3.
We accomplished this through heavily optimised code and algorithms.
It also requires a single \MSA{} and no external predictors, which makes it easy to install, deploy, and apply in large-scale studies.

Our method is based on a combination of \DCA{} (a faster re-implementation of Gauss\DCA{}), mutual information, and other statistics derived from the \MSA, combined and refined by a convolutional neural network (\CNN).
In order to train as big a network as possible, we used the U-net architecture, which allows for large receptive fields and number of parameters on the limited memory of our \textsc{gpu}s.
Our training data is taken from \textsc{pisces}~\citep{pdbcull}.

In order to maximise the information from our training data, we used multi-task learning (see Section~\ref{sec:transfer}) to predict contacts at 6, 8, and \SI{10}{\angstrom}, as well as the distance between residues under the transformation $1/(d + \SI{20}{\angstrom})$.

At the time of publication, it was the best freely available contact predictor that could use a single \MSA.


\section*{Paper \textcolor[cmyk]{0, 0.87, 0.68, 0.32}{IV}}
\begin{center}
	\textsc{A novel training procedure to train deep networks in the assessment of the quality of protein models.}
	\emph{Manuscript}
\end{center}

\noindent
The power of deep learning lies in its ability to leverage the inherent structure of the data.
In this work, we show how we can take this one step further, and bake the structure of the \emph{problem} in the network architecture, introducing ProQ4.
We demonstrate it is a viable alternative reaching or surpassing ProQ3D's performance (from Paper \textcolor{Maroon}{I}) on the same training and test sets, but  using only a subset of the inputs, following the line of simplicity outlined in PconsC4, and no external predictors.

ProQ4 was designed as a coarse-grained program with a focus on model selection and ranking.
Coarse-grained means we are not overly sensitive to finer details of a model, such as the side chain packing, which is relatively unimportant for \MQA, yet may change widely from group to group.
The last step of a protein structure prediction is to select the best model from our pool.
An improvement in model selection will impact positively any protein structure prediction pipeline by selecting better candidates, and with more confidence.

In this paper we made extensive use of transfer learning, as explained in Section~\ref{sec:transfer}, in both multi-task and pre-training to extract the maximum amount of information and minimise the impact of bias.
In particular, we have developed a model that can be used to both predict secondary structure from a \MSA, and learn a representation of said \MSA{} that can be applied for any other deep learning tasks.


\section*{Paper \textcolor[cmyk]{0, 0.87, 0.68, 0.32}{V}}
\begin{center}
	\textsc{Estimation of model quality accuracy in \CASP13.}
	\emph{Proteins}, 2019. (In press).
\end{center}

\noindent
This paper presents the results on model quality assessment of the latest edition of the blind test Critical Assessment of (\textsw{Protein}) Structure Prediction, \CASP13,
and serves as independent validation for the results in Papers \textcolor{Maroon}{I} and \textcolor{Maroon}{IV}.

We can see that indeed, the results are better on the loss selected for training.
The different ProQ3D-\textsc{xx} variants -- where \textsc{xx} stands for different target functions, but using the same training data, model, and features  -- performs best on the target function was trained on.

ProQ4 has a better ranking than ProQ3D, but has poor performance in local scores.
We believe this is because of the limited description of the protein, using only the properties of the backbone.
Further research should include physico-chemical and finer-grained descriptors that could add orthogonal information.
