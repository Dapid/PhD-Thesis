\chapter{Deep learning}
\section{Success stories}
We start this section introducing the two fields that have been spearheading the Deep Learning revolution: computer vision and speech recognition.
The first is 
\todo{perceptual learning}

\section{The basic blocks}
\section{Taming the complexity: regularisation}
Neural networks can have millions of parameters, so they are susceptible to over-fitting.
In order to converge to generalisable models, we can apply a variety of regularisation techniques.

In general, they are a barrier that hinders the training, that will only be overcame if enough data supports it.
Here are some:

\marginpar{Weight decay}
To prevent any single activation from

$L^2$ regularisation can be interpreted as a Gaussian prior over the weights centred around $0$.

The most popular technique \marginpar{Dropout} 
specifically developed for deep learning is Dropout, \citep{dropout}. 
During training, a random fraction $0 < \rho < 1$ of intermediate inputs is set to $0$, while the rest of values are scaled by a factor of $\frac{1}{1-\rho}$ to compensate.

Since 

\marginpar{Batch Normalisation}

\marginpar{Differential privacy}
Special mention

\marginpar{Architectural}

\section{The quest for depth}
b
\section{Deep transfer learning}
a


