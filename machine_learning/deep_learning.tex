\chapter{Deep learning}
\section{Blocks}
\section{Taming the complexity: regularisation}
Neural networks can have millions of parameters, so they are susceptible to over-fitting.
In order to converge to generalisable models, we can apply a variety of regularisation techniques.

In general, they are a barrier that hinders the training, that will only be overcame if enough data supports it.
Here are some:

\marginpar{Weight decay}
To prevent any single activation from

$L^2$ regularisation can be interpreted as a Gaussian prior over the weights centred around $0$.

\marginpar{Dropout}
The most popular technique specifically developed for artificial neural networks is Dropout, \citep{dropout}.
During training, a random fraction $0 < \rho < 1$ of intermediate inputs is set to $0$, while the rest of values are .

\marginpar{Batch Normalisation}

\marginpar{Differential privacy}
Special mention

\marginpar{Architectural}

\section{The quest for depth}
b
\section{Deep transfer learning}
a
