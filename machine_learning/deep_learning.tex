\chapter{Deep learning}
\section{Success stories}
We start this section introducing the two fields that have been spearheading the Deep Learning revolution: computer vision and speech recognition.
The first consists in identifying objects in digital images; while the second is being able to transcribe and process audio.

It is striking that both cases correspond to tasks that humans are very good at.
Indeed, neural networks are very good at \emph{perceptual learning}

\section{The basic blocks}
A deep learning model is composed by a series of layers, or simple transformations (usually linear), plus a point-wise non-linearity.


\subsection{Fully connected layers}
Also known as ``dense" layers, they are the simplest building block of deep learning: a simple matrix multiplication, as described in the Section~\ref{sec:mlp}.
To make them more general, they can include a \emph{bias} term, that is added after the multiplication.

\[
\vec{o} = W \cdot \vec{i} + \vec{b}
\]

Every element of the output is a function of every input, so this layer destroys the structure of the data.
On the other hand, since it can have a lot of parameters, it can be very powerful.
It is often used as the last layer of the model.

\subsection{Convolutions}

\subsection{Recurrent}
\subsection{Non-linearities}

\section{Conjunctive tissue: tensors and gradients}

\section[Gradient descent]{Training procedure: gradient descent}\label{sec:grad_descent}
\subsection{Backpropagation}
\subsection{Stochastic gradient descent}

\section{Taming the complexity: regularisation}
Neural networks can have millions of parameters, so they are susceptible to over-fitting.
In order to converge to generalisable models, we can apply a variety of regularisation techniques.
Most of them act as a barrier that hinders the training, that only enough data can overcome.


Here are some:


To prevent any single activation from \marginpar{Weight decay}

$L^2$ regularisation can be interpreted as a Gaussian prior over the weights centred around $0$.

The most popular technique \marginpar{Dropout} 
specifically developed for deep learning is Dropout, \citep{dropout}. 
During training, a random fraction $0 < \rho < 1$ of intermediate inputs is set to $0$ (\emph{dropped out}), while the rest of values are scaled by a factor of $\frac{1}{1-\rho}$ to compensate.

Since the network cannot trust any particular neuron activation to be present, it must distribute the information across different parts.
The architecture is effectively different for every batch, so we are training an ensemble of models, most of which have not seen any data, but are heavily regularised to the average.

\marginpar{Batch Normalisation}

\marginpar{Architectural}

\marginpar{Differential privacy}
Special mention



\section{The quest for depth}
%relu
%batchnorm
%residual

\section{Deep transfer learning}
a


