\chapter{Deep learning}
\lettrine[lines=3, lhang=0.25, nindent=0em, findent=3pt]{\color{Maroon}W}{e saw in the}
previous chapter how to turn simple linear models into more general multi-layered perceptrons via stacking.
Deep learning is the logical continuation to a higher number of layers.
This chapter discusses why we would want to do it and how to train them.

\section*{Success stories}
We start this section introducing the two fields that have been spearheading the Deep Learning revolution: computer vision and speech recognition.
The first consists in identifying objects in digital images; while the second is being able to transcribe and process audio.

It is striking that both cases correspond to tasks that humans are very good at.
Indeed, neural networks are very good at \emph{perceptual learning}, or tasks where the computer replaces the human senses.
This is perhaps because neural networks are inspired by the biological nervous system, but also because humans can understand the process,
label more data, and guess at why it is wrong.
Furthermore, they are \emph{solvable} problems.

But we are not only limited to perceptual problems.
Deep neural networks have shown to be good at tasks that humans cannot perform well, like contact prediction~\citep{ultra_deep_contacts}, estimation of protein model quality~\citep{casp13_ema}, and other tasks.

%The rest of this chapter will contain a summary of the fundamental blocks, how to train, 

\section{The basic blocks}
A deep learning model is composed of a series of layers, or simple transformations -- usually linear -- plus a point-wise non-linearity.


\subsection{Fully connected layers}
Also known as ``dense" layers, they are the simplest building block of deep learning: a simple multiplication of the input $\vec{i}$ with a matrix $\widetilde{W}$ whose entries we want to determine, as described in Section~\ref{sec:mlp}.
To make them more general, they can include a \emph{bias} term, $\vec b$, that is added after the multiplication.

\begin{equation*}
\vec{o} = \widetilde W \cdot \vec{i} + \vec{b}
\end{equation*}

Every element of the output vector $\vec o$ is a function of every input, so this layer partially destroys the structure of the data.
On the other hand, since the intermediate layers can be arbitrarily large, it can have a lot of parameters, ie, it can be very expressive.
It is often used as the last layer of the model.

\subsection{Convolutions}
A convolution is a linear operation over a small patch, that is repeated across the whole input, to  capture spatial relationships in the data via keeping translational invariance.
\marginpar{1D}
Consider the case of 1D convolutions.
The input is a 2D matrix, where we have a number of input channels at each position.
A convolution looks at a small but fixed window and applies a linear operation; then it moves one unit and repeats the same operation, with the same weights as before.
The output is another 2D vector, with an arbitrary number of channels, which outputs are mutually independent.
The set of weights corresponding to each output is called a \emph{filter}.

Each convolution  \marginpar{Receptive fields} has a limited field of view, that can be increased stacking layers.
 

\subsection{Recurrent}
A recurrent layer tries to capture temporal dependencies on arbitrary time steps, such as those of natural language, or repeating motifs in proteins.
They work by taking two inputs: the vector at the current position ($\vec x_t$), and the hidden state of the previous cell ($\vec h_{t-1}$).
A simple cell has the form:

\begin{align*}
\vec h_t &= \sigma\left(\widetilde W_{hh} \vec{h}_{t-1} + \widetilde W_{xh} \vec{x}_{t}\right) \\
\vec y_t &=  \widetilde W_{hy} \vec h_t,
\end{align*}
where $ \widetilde W_{\cdot \cdot}$ are three matrices and $\sigma$ a non-linear function, usually the hyperbolic tangent.
\marginpar{\RNN{} as a rolled \MLP}
Note that this structure is very similar to a fully connected layer with two inputs, where the weights are shared across steps.
The series of outputs at every time step $\vec y_t$ can be kept, to get a different prediction at every time step, or only considered the last one if we want a global one.

This layer has practical problems. \marginpar{Convergence issues}
In particular -- as we will see in the next section -- if the non-linearity is saturating and any of the intermediate steps reaches the saturating region, the gradients will vanish, preventing the flow of information.
If instead, we use a non-saturating non-linearity we risk having an unstable dynamical system, where the values explode.
The most popular solution \marginpar{\LSTM} is the Long Short Term Memory (\LSTM) layer, that will be explained in Section~\ref{sec:depth}.


\citet{unreasonable_RNN} has a thorough introduction to Recurrent networks, and \citet{colah_rnn} delves deeper into the convergence issues in long-range, and the solutions.

\subsection{Non-linearities}
The workhorses of deep learning layers are linear transformations.
The composition of linear transformations is also linear, so to reap the benefits of multiple layers, we need non-linearities.
For the most part, they are point-wise functions that act on the output of a layer.
Historically, the first non-linearity used was the sigmoid:

\begin{equation*}
f(x) = \frac{1}{1 + e^{- x}}
\end{equation*}

It has the advantage of clipping the range of values between 0 and 1, which makes the network stable with respect to large intermediate values (Figure~\ref{subfig:sigmoid}).
An improvement was the hyperbolic tangent:

\begin{equation*}
f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}},
\end{equation*}
with a similar shape to the sigmoid, but now outputting values between -1 and 1. (Figure~\ref{subfig:tanh})
Being anti-symmetric, it can achieve a stable distribution of outputs with normally distributed weights in the intermediate layers.

\begin{figure}[tb]
	\subcaptionbox{Sigmoid\label{subfig:sigmoid}}{\includegraphics[width=0.45\textwidth]{machine_learning/figures/sigmoid}}
	\hfil
	\subcaptionbox{Tanh\label{subfig:tanh}}{\includegraphics[width=0.45\textwidth]{machine_learning/figures/tanh}}
	\caption{Saturating non-linarities}\label{fig:non_linear}
\end{figure}

Both functions saturate on extreme values, \marginpar{Gradient saturation}
i.e. their derivative approaches zero.
While this is useful to control the stability of the outputs, it is a problem for the propagation of gradients.
The alternative is using non-saturating functions, that allow the gradients to flow for a wider range of values.

The simplest is the Rectified Linear Unit, or ReLU, introduced by~\citet{relu} and plotted in Figure~\ref{subfig:relu}:
\begin{equation*}
f(x) =  \begin{cases}
\hfil x &  \text{if } x \geq 0 \\
\hfil 0 & otherwise
\end{cases}
\end{equation*}

But in this thesis I have mostly used the Exponential Linear Unit, or \ELU, shown in Figure~\ref{subfig:elu}:
\begin{equation*}
f(x) =  \begin{cases}
\hfil x &  \text{if } x \geq 0 \\
\frac{1}{1 + e^{-x}} & otherwise
\end{cases}
\end{equation*}

The choice of this non-linearity incurs in a slight increase in training time, since it needs to compute the exponential; but it usually gives a moderate improvement in performance as well.

\begin{figure}[tb]
    \centering
	\subcaptionbox{ReLU\label{subfig:relu}}{\includegraphics[width=0.45\textwidth]{machine_learning/figures/relu}}
	\hfil
	\subcaptionbox{\ELU\label{subfig:elu}}{\includegraphics[width=0.45\textwidth]{machine_learning/figures/elu}}
	\caption{Non-saturating non-linarities}\label{fig:non_linear2}
\end{figure}

The last layer is an exception.\marginpar{Output}
The function applied depends on the properties of the target:
\begin{itemize}
\item If the output is a binary classification or multiple mutually compatible classes, use a sigmoid, as described in Section~\ref{sec:logistic_regression} on logistic regression.
\item If the output is a series of mutually exclusive classes, use a softmax, as explained in Section~\ref{sec:mlp} on multi-layered perceptrons.
\item If the output is a regression, use a linear layer to minimise the squeezing of the outputs.
This is valid even if the output is bounded between 0 and 1, when the values at the extrema are likely to appear because the network would need to learn very high values for the logits.
\item If the outputs are angles, split the labels into sines and cosines and use the hyperbolic tangent.
This representation preserves angular distances -- ie, angles that differ by almost $2 \pi$ are close -- and are not biased towards an arbitrary middle point.
The original angle can be recovered using the $\arctantwo$ function.
\end{itemize}

\section[Gradient descent]{Training procedure: gradient descent}\label{sec:grad_descent}
A deep learning network can have millions of parameters, but how do we find the optimal values?

First, we need to define a loss function, that is, a measurement of how wrong our predictions are on known data.
For example, the mean squared error for $N$ data points  $x_i$ with labels $y_i$ and predictions $o_i$ is:

\begin{equation*}
L_{\MSE} = \sum_i^N \left(y_i - o_i\right)^2
\end{equation*}

Another popular choice is the cross-entropy loss, that measures the information loss between true and predicted values:
\begin{equation*}
L_{H} = \sum_i^N y_i \cdot \log\left(o_i\right) 
\end{equation*}

We initialise the network with random numbers and measure how wrong we are.
Then, we compute the gradients of the loss with respect to each of the parameters $w_j$ of the network:
\begin{equation*}
g_j = \frac{1}{N}\sum_i^N\frac{\partial L\left(o_i, y_i \right)}{\partial w_j}
\end{equation*}

The gradients tell us in which direction we have to ``nudge" the network to improve its performance for the next iteration:

\begin{equation*}
\left. w_j\right|_{it=k+1} = \left. w_j\right|_{it=k} - \eta \cdot \left. g_j\right|_{it=k}
\end{equation*}
where $\eta$ is the step size, a small number that reflects our belief of the size of the region where the gradients still point in the same direction.

\subsection{Back-propagation}
To train the network, we need to compute the gradients of the loss with respect to all the parameters of the network.
This can be done automatically using the back-propagation algorithm, or \emph{backprop} \citep{backprop}, the machine version of the chain rule.

We start computing the derivative of the loss with respect to the output of the network ${\partial L}/{\partial o_i}$.
Then we can apply the chain rule to compute the derivatives with respect to the output $y_i$ before the non-linearity:
\begin{equation*}
\frac{\partial L}{\partial y_i} = \frac{\partial L}{\partial o_i} \frac{d o_i}{d y_i}
\end{equation*}

For example, if the last layer is a fully connected layer without a bias, the gradients for each weight are:

\begin{equation*}
\frac{\partial L}{\partial w_{ij}} = \frac{\partial L}{\partial y_j} \frac{\partial y_j}{\partial w_{ij}} =  \frac{\partial L}{\partial y_j}  w_{ij}
\end{equation*}

The procedure \marginpar{Computational graph} can be applied recursively to an arbitrary number of layers using a computational graph: a directed graph where the nodes are inputs, outputs, and intermediate values; and the edges connect each node with the values that are immediately needed to compute it.
Figure~\ref{fig:comp_graph} contains a simple example.
The differentiation can be done automatically, so the deep learning practitioner can write models without needing to explicitly write down the gradients, which is time-consuming and error-prone.

\begin{figure}[bht]
	\centering
	\input{machine_learning/figures/comp_graph.tikz} %
	\caption{An example of a computational graph computing the simple formula above.
	The square nodes are the three inputs, $a$, $b$, and $c$; while the diamond indicates the output value, $\mathcal{O}$.
    The node in italics, $\mathit{d}$ is a temporary variable.}\label{fig:comp_graph}
\end{figure}


Computing the outputs \marginpar{Backward and forward pass}
of the network is called \emph{forward pass}, since the output of each layer depends only on the previous values, closer to the inputs.
To compute the gradients, we need to keep the outputs of each layer between the weight and the labels.
Since we transverse the graph in the opposite direction, it is called the \emph{backward pass}.

\subsection{Stochastic gradient descent}\label{sec:sgd}
Computing the backward pass takes roughly 3 times longer than the forward pass~\citep{dl_course}.
Since deep learning models require large amounts of data, computing the gradients on the whole dataset is impractical.
Instead, we can approximate the gradients of the loss w.r.t the whole data by a random sample:

\begin{equation*}
g_j = \frac{1}{N} \sum_i^N \frac{\partial L\left(o_i, y_i\right)}{\partial w_i} \simeq \frac{1}{n} \sum_i^n \frac{\partial L\left(o_i, y_i\right)}{\partial w_i}
\end{equation*}
where $n << N$.
Each subsample is called a \emph{mini-batch}, \marginpar{Minibatch}
and it is re-sampled after every iteration.
Once we have sampled the whole data, we have completed an \emph{epoch}.

The stochasticity of the gradient descent introduces noise in the gradients.
While this may slow down convergence, it also helps the network explore the parameter space, allowing it to break from possible local minima.

\subsection{Optimisers}
The optimisers are the algorithms responsible for transforming the gradient into weights updates.
The simplest form is the already introduced Stochastic Gradient Descent (\SGD), \marginpar{\SGD}
where the weight updates are proportional to the gradient computed at the previous iteration:

\begin{equation*}
\left.\Delta w_j\right|_{it=k+1} = - \left.\eta g_j\right|_{it=k} = - \eta \left.\frac{\partial L}{\partial w_j}\right|_{it=k}
\end{equation*}

This is the basis for all optimisers used in Deep Learning.


To reduce the noise introduced by the stochasticity we can introduce a momentum term, regulated by the parameter $\mu$:

\begin{align*}
&\left.v_j\right|_{k+1} = \mu \left. v_j\right|_{k} - \left.\eta g_j\right|_{it=k}\\
&\left.\Delta w_j\right|_{it=k+1} = \left.v_j\right|_{k+1}
\end{align*}

The ``velocity" vector $v_j$ keeps a memory of the latest updates.
The ``mass" $\mu$ controls the balance between the directions pointed at by new updates and keeping the inertia.

The momentum term \marginpar{Nesterov}
may be pointing in the wrong direction, for example, because we have overshot the minimum.
The Nesterov momentum update~\citep{nag} performs the momentum update before computing the gradients, giving the gradient the possibility of correcting mistakes if the momentum was misleading.
If the momentum term was instead a good estimation, the new gradient will be computed closer to the minimum, yielding a better estimation.

In similarity \marginpar{Learning rate schedule}
to the temperature in Simulated Annealing \citep{genSA}, we can reduce the learning rate~$\eta$ as the training progresses.
A common choice is to reproduce cooling down the system as reducing the learning rate.
As in thermodynamics, a smaller temperature means the system has to move more slowly, giving smaller steps, and exploring the space closer to the current position.
On the $k-$th epoch, the learning rate is:

\begin{equation}
\eta= \frac{\eta_0}{1 + \delta \cdot k}
\end{equation}

Another option is to not set the schedule upfront, but adapt it to the specific training.
I.e. reduce it by a fixed fraction when the loss has not improved for a certain number of epochs.
We believe the system to be close to the minimum, and we need to take smaller steps to approach it.

Instead of setting a learning rate schedule from the beginning, adaptive optimisers try to adjust it to the data.
The fundamental idea is that the closer we are to the minimum in each dimension, the smaller the learning rate should be.

AdaGrad\marginpar{AdaGrad} (Adaptative Gradients~\citep{adagrad}) uses the $L^2$ norm of the gradients to regulate the learning rate.
We drop the index subscripts, as every term corresponds to one dimension:

\begin{align*}
&\left.a\right|_{k+1} = \left. a\right|_{k} + \left.g^2\right|_{it=k}\\
&\left.\Delta w\right|_{it=k+1} = - \eta\frac{g}{\sqrt{a + \epsilon}} =  - \eta\frac{g}{\sqrt{\sum_{\tau=0}^{k} \left.g\right|_\tau + \epsilon}}
\end{align*}
where $\epsilon$ is a small number to prevent division by 0.
The effective learning rate of AdaGrad is monotonously decreasing, and it decays faster in the directions where the gradients are higher.
This makes it sensitive to initial conditions: if any dimension is given a bad initial condition, the gradients will be large, and the effective learning rate small, so AdaGrad will be limited to only small steps in that direction.
\ADADELTA~\citep{adadelta} solves this by accumulating only the updates over a window with an exponential decay:

\begin{equation*}
\left.a\right|_{k+1} = \rho \left. a\right|_{k} + (1-\rho) \left.g^2\right|_{it=k},
\end{equation*}
allowing the algorithm to overcome past large gradients.
It also considers the dimensionality of the quantities: in AdaGrad the learning rate has dimensions.
They replaced it for a value of the right dimensionality:

\begin{align*}
\left.\Delta w\right|_{it=k+1} = - \frac{\sqrt{\left.\Delta w\right|_{k}^2}}{\sqrt{a + \epsilon}}g 
\end{align*}

\ADADELTA{} depends only on one parameter, $\rho$, and it is less sensitive to starting points.

Adaptive Moment Estimation (Adam)~\citep{adam}\marginpar{Adam}
is the most modern optimiser, and it combines several of the ideas of previous algorithms.
In the first place, it keeps track of the two moments of the distribution of the gradient on a sliding window with decay rates $\beta_1$ and $\beta_2$:

\begin{align*}
&\left.m\right|_{k+1} = \beta_1 \left.m\right|_k + (1-\beta_1) \left.g\right|_{k+1}\\
&\left.v\right|_{k+1} = \beta_1 \left.v\right|_k + (1-\beta_1) \left.g^2\right|_{k+1}
\end{align*}

Since they are initialised with 0s, they are biased, especially at the beginning. We can correct them:

\begin{align*}
	&\left.\hat{m}\right|_{k+1} = \frac{\left.m\right|_{k+1}}{1-\beta_1^{k+1}}\\
	&\left.\hat{v}\right|_{k+1} = \frac{\left.v\right|_{k+1}}{1-\beta_2^{k+1}}
\end{align*}

The final update rule is:

\begin{equation*}
\left.\Delta w\right|_{k+1} = - \frac{\eta}{\sqrt{\left.v\right|_{k+1}} + \epsilon}\left.\hat{m}\right|_{k+1}
\end{equation*} 

This is the optimiser that will be used in the rest of the thesis.
For a thorough review consult the work of \citet{optimisers_review}.


\section{Taming the complexity: regularisation}
Neural networks can have millions of parameters, so they are susceptible to over-fitting.
In order to converge to generalisable models, we can apply a variety of regularisation techniques.
Most of them act as a barrier that hinders the training, in a way that only enough data can overcome.
Others encode invariances of the real world into our model, such as translation invariance given by convolutions on images.
Here are some:


To prevent any single activation from \marginpar{Weight decay}
dominating the predictions, we can add a term to the loss penalising large weights.
If this term is proportional to the square of the weights, it is called $L^2$ regularisation and can be interpreted as a Gaussian prior over the weights centred around $0$, as seen with linear models in Section~\ref{sec:linear}.

The most popular technique \marginpar{Dropout} 
specifically developed for deep learning is Dropout,~\citep{dropout}. 
During training, a random fraction $0 < \rho < 1$ of intermediate inputs is set to $0$ -- \emph{dropped out} --  while the rest of values are scaled by a factor of $\frac{1}{1-\rho}$ to compensate.
Since the network cannot trust any particular neuron activation to be present, it must distribute the information across different parts.
The network is effectively different for every batch, so we are training an ensemble of models, most of which have not seen any data, but are heavily regularised to the average.
At prediction time, dropout is usually turned off, but if we leave it on and sample with the same input, we can estimate the uncertainty of the prediction like in Gaussian Processes.

The next breakthrough in regularisation is Batch Normalisation, \marginpar{Batchnorm}
or Batchnorm~\citep{bn}.
This has the unusual properties of both increasing generalisation capabilities -- thus serving as a regulariser -- and helping training performance -- ergo, without hindering.

It works as a layer that learns the mean and standard deviation of the distribution of its inputs and corrects them to keep them close to 0 and 1, respectively.
As was seen with some optimisers, it can keep a momentum term to smooth out the changes in averages.
In the original paper, it was placed before the activation, but further research suggests it is better to place it after it.

The reason why Batch Normalisation actually works as a regulariser was not well understood until the work of \citet{how_bn_works}, who showed how it smooths the loss function landscape, both helping the training and converging to more generalised minima.
\citet{large_batch} showed that smoother landscapes lead to better generalisation.

On a higher level,\marginpar{Architectural}
we can design our network to be robust with respect to invariances in our data.
For example, convolutions are translational invariant, so a convolution-based secondary structure predictor will be able to recognise the sequence signature of an $\alpha$-helix regardless of its relative position.
We can also  design our training and architecture to fit the problem, as we did on Paper \textcolor{Maroon}{IV}.

Sometimes, \marginpar{Data augmentation} we wish our network to learn an invariance that cannot be embedded in the network architecture or the representation, such as scales of natural images in computer vision, or multiple sequence alignment quality in protein bioinformatics.
Since deep learning models can leverage arbitrarily large datasets, we can use \emph{data augmentation}, a procedure where data points at training time are modified in the way we want our network to learn invariance.
So, in computer vision by showing the image of the same cat at multiple sizes and orientations, we hope our network can recognise a cat regardless of the two transformations.
In bioinformatics, we may want to train on alignments generated with different settings to provide more robust networks.

Special mention deserves the concept of \emph{Differential Privacy}, \marginpar{Differential Privacy}
or how to learn from sensitive data without leaking private information~\citep{differential_privacy}.
This framework allows users to train models with the confidence that an adversary with full access to the trained model cannot recover any single identifiable information because the model is guaranteed not to memorise any of its training data.

One procedure implemented by \citet{tf_privacy} starts by splitting each mini-batch used by the Stochastic Gradient Descent into micro-batches and computing the gradient for each of them.
Then, the gradients are clipped, so that any micro-batch can provide only so much change in the weights, and independent random Gaussian noise is added to each of them, to dilute any private signal they may have.
At the end of the iteration, the mini-batch is averaged, and the weights updated.


\section[Tensors and gradients]{Conjunctive tissue: tensors and gradients}\label{sec:tensors}
As we have seen before, each layer is composed of mostly tensors\footnote{In the Computer Science definition: a hypermatrix of arbitrary dimensions, such as 1-D vectors, 2-D matrices, or 3-D volumes.} and linear operations.
Every intermediate layer in a deep network is learning a transformation of its inputs, that we can interpret as a representation that brings it closer to the target.
These representations are, in general, difficult for humans to interpret, but in some cases, like computer vision, we can \emph{visualise} them.
A convolutional network trained to identify objects in images will learn to recognise edges and simple textures in the first layers, that are combined into more complex representations as the information travels through the network.

One important consequence is that all data, regardless of its source or nature, is represented as tensors.
We can then seamlessly connect through these tensors different branches working with dissimilar kinds of data, and train end-to-end having them talk to each other through gradients.
\citet{tensors_and_gradients} has a pedagogical explanation on this idea applied to automatic description of images in natural images.
A more extreme example is the work of \citet{one_rule_all}, where they combine eight different tasks into a single network, including translation, captioning, and image recognition.


\section{The quest for depth}\label{sec:depth}
Deeper networks have advantages over shallower ones, even for the same number of parameters.
For example, stacking convolutions with a few filters each means the final layer has a bigger receptive field than a single convolution with many filters.
Furthermore, most layers are ``stupid" linear transformations with non-linearities between them.
Stacking more layers means we obtain a more non-linear function, and hence, richer.
The main practical obstacle to train deeper networks is to be able to transmit the gradient information down to every layer.
Indeed, in the first attempts with multi-layered perceptrons, more than two hidden layers did not offer an advantage.
\citet{glorot} studied in detail the difficulties of training multi-layered perceptrons known at the time.

Their first contribution \marginpar{ReLU, again} was non-saturating non-linearities, like ReLU.
Since they do not saturate, neurons are never stuck in ranges where the gradients are almost zero, and thus prevented from learning.
\marginpar{\scriptsize ReLU has the advantage of providing a sparse intermediate representation, which is useful for information disentangling, ie, inferring the causes of the prediction, since perturbing the inputs do not affect all the outputs.}
ReLU has a null gradient for negative numbers, but we expect that to happen only half of the examples, while half of the others will give a positive signal.
This is not guaranteed, and there is a chance of ``dead neurons", where the activation is always zero, but that has not been found to be a significant problem.
If dead neurons are a problem, LeakyReLU  \marginpar{LeakyReLU} can be the solution.
This activation is like ReLU, but the negative side has a small, fixed slope, so the gradient is non-zero everywhere.


Tightly tied \marginpar{Weight initialisation} with the non-linearity is the initialisation of the weights.
Glorot et al. recommended initialising the layers with zero mean and standard deviation $\sigma=6/\sqrt{n_{in}+ n_{out}}$, where $n_{in}$ and $n_{out}$ are, respectively, the number of input and output features of the layer.
In this case, small initialisations are critical because saturating neurons are very slow to train.
With non-saturating initialisations like ReLU, we can use larger weights without fearing the plateaus.
\citet{he} recommends $2/\sqrt{n_{in}}$ as a stable choice: each output will be a linear combination of $n_{in}$ input features.
Since the standard deviation of the sum of $n$ normal random variables is proportional to $\sqrt{n}$, and half the activations are zero, this factor keeps the distribution of activation stable across layers.

Adapting the initialisation \marginpar{Batchnorm} to the non-linearity uses the expected dynamics of a random network to keep stable distributions.
Batch normalisation forces them, ensuring that the information can flow regardless of the dynamical state of the network, and adapting to the particular features of the dataset.

If the first layers \marginpar{Auxiliary outputs} in a deep model are
too far from the labels, can we bring them closer, without sacrificing depth?
\citet{googlenet} attempted that in GoogleLeNet, where the network had several output layers at different depths.
While only the last one was actually used in production, the intermediate ones inject gradients and help train the lower layers.

The last fundamental idea \marginpar{ResNets} is Residual Networks~\citep{resnet}, which allows to train networks of arbitrary depth.
ResNets are composed of blocks of two layers  -- convolutions in the original paper, but applicable to any other type -- where the output is directly added to the inputs using a so-called \emph{skip connection}, as illustrated in Figure~\ref{fig:resnet}.
This extra path opens an information highway from the inputs to the labels.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.45\textwidth]{machine_learning/figures/resnet}
	\caption{The Resnet block as appeared in the publication.
	Note that placing the BatchNorm after the activation is usually advantageous.}\label{fig:resnet}
\end{figure}

Recurrent networks, \marginpar{Recurrent networks, revisited} albeit formally ``shallow" -- they typically have one or two layers -- face similar problems to feed-forward networks, now because the information has to travel along the time dimension.
Long Short Term Memory (\LSTM) layers are a variation of the simple \RNN{} that includes some internal skip connections, allowing the network to transfer long-range information, but also ``forget" gates that can learn to ignore certain parts of the input that may be irrelevant.

\citet{difficult_recurrent} explored the influence and solutions of initialisations and non-linearities to the problems of vanishing and exploding gradients, and provide recipes for training recurrent networks minimising these problems.


\section{Deep transfer learning}
The success of machine learning is tightly coupled to the quality and quantity of both the training data and the labels.
The concept of \emph{transfer learning} can help us mitigate to some extent these limitations by leveraging related datasets and extracting knowledge.
It comes in two flavours: multi-task, and representation learning.

\subsection{Multiple tasks}
Some tasks are related to each other, so a model that learns them simultaneously will perform better than when done independently.
\marginpar{A worked example}
Figure~\ref{fig:transfer_learning} shows the Ramachandran plot,  $\phi$ and $\psi$ dihedral angles, for a set of proteins.
The points are clustered in distinct regions, suggesting that knowledge of one of the angles also conditions the other.
An algorithm that learns them together has a better chance of recovering this relationship than two distinct models without information from each other.
Furthermore, the dihedral angles are conditioned by secondary structure, so for example, knowing a residue is in an $\alpha$-helix means the $\psi$ angle is close to zero.
Lastly, we can observe that the surface accessibility is not completely conditioned by the dihedral angles, but they are not independent.
Indeed, $\alpha$-helices are more likely to be exposed than $\beta$-sheets, but rarely more than 60\%.

The ways multi-task learning \marginpar{The general case} can help our predictions are:
\begin{itemize}
	\item Implicitly learn of the joint distributions of dependent outputs.
	\item Some tasks are difficult.
	Having labels for an easier task can help bring the network to a state where it is easier to predict the harder one.
	\item Even if the tasks are equally hard, combining gradients from several tasks can help reduce noise and improve convergence.
	\item Labels from different annotations mean we have more data, which means we can train a larger and more expressive model.
	%In the extreme case where we have two tasks completely unrelated, the network will converge into two separated sub-networks, so we can train a network that is, ideally, as big as the sum of the ideal network for each of the two tasks.
	%In the practical case, where tasks are indeed related, a significant fraction of the network is ``recycled", so each section is effectively bigger.
\end{itemize}

This was the idea behind the secondary structure predictor used in Papers \textcolor{Maroon}{III} and \textcolor{Maroon}{IV}.

\begin{figure}[bht]
\centering
\input{machine_learning/figures/transfer_learning_annotated.tikz} %
\caption{Plot of the $\phi$ and $\psi$ dihedral angles for a random set of ten proteins.
The colouring correspond to their relative exposure to solvent (\RSA),
and the annotated clusters correspond to the most common secondary structure elements.
The figure shows how the three tasks, albeit independent, can help each other. }\label{fig:transfer_learning}
\end{figure}

\subsection{Representation learning}
As explained in Section~\ref{sec:tensors}, each intermediate layer in a deep network is learning a representation of its inputs, a transformation that brings it closer to the target.
If the dataset for our task of interest is small or biased, our network will not be able to learn a good representation, ergo the performance will be poor.
In this situation, we can apply transfer learning, which is done in two steps:
\begin{enumerate}
\item \emph{Pre-training:} train a network on a related task for which we have a high-quality dataset.
This step learns a new representation of the data.
\item \emph{Training:} replace the top layers with new ones, and retrain on the dataset and task of interest while keeping the weights of the re-used layers unchanged.
We keep our previous representation but use it for a new task.
This step is training fewer parameters, so it needs less data, and the architecture can be more constrained.
\item \emph{Fine-tuning (optional):} we can refine the predictions by allowing more layers to change.
In this stage, we unfreeze some of the pre-trained layers, and train again.
Our network is already trained, so we only want to take small steps.
This can be done keeping the learning rate small. Otherwise, we will ruin the pre-training.
\end{enumerate}

Paper \textcolor{Maroon}{IV}, makes use of this technique, where we pre-trained a section of the network on a large subset of the PDB to predict secondary structure from sequence.
These predictions were then used by a second network, combined with the secondary structure of models, to predict their quality.

