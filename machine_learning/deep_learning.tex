\chapter{Deep learning}
\section{Success stories}
We start this section introducing the two fields that have been spearheading the Deep Learning revolution: computer vision and speech recognition.
The first consists in identifying objects in digital images; while the second is being able to transcribe and process audio.

It is striking that both cases correspond to tasks that humans are very good at.
Indeed, neural networks are very good at \emph{perceptual learning}

\section{The basic blocks}
\subsection{Fully connected layers}
Also known as ``dense" layers, they are the simplest 

\subsection{Convolutions}
\subsection{Recurrent}
\subsection{Non-linearities}

\section{Conjunctive tissue: tensors and gradients}

\section{Training procedure}
\subsection{Backpropagation}
\subsection{Stochastic gradient descent}

\section{Taming the complexity: regularisation}
Neural networks can have millions of parameters, so they are susceptible to over-fitting.
In order to converge to generalisable models, we can apply a variety of regularisation techniques.

In general, they are a barrier that hinders the training, that will only be overcame if enough data supports it.

Here are some:

\marginpar{Weight decay}
To prevent any single activation from
$L^2$ regularisation can be interpreted as a Gaussian prior over the weights centred around $0$.

The most popular technique \marginpar{Dropout} 
specifically developed for deep learning is Dropout, \citep{dropout}. 
During training, a random fraction $0 < \rho < 1$ of intermediate inputs is set to $0$ (\emph{dropped out}), while the rest of values are scaled by a factor of $\frac{1}{1-\rho}$ to compensate.

Since the network cannot trust any particular neuron activation to be present, it must distribute the information across different parts.
The architecture is effectively different for every batch, so we are training an ensemble of models, most of which have not seen any data, but are heavily regularised to the average.

\marginpar{Batch Normalisation}

\marginpar{Differential privacy}
Special mention

\marginpar{Architectural}

\section{The quest for depth}
b
\section{Deep transfer learning}
a


