\chapter{Machine learning}
%\section{What is machine learning?}

The Scientific Revolution during the Renaissance \marginpar{A historical note}
was fuelled by the realisation that Nature can be parametrised using only a handful of equations.
With these tools, and sufficient measurements, natural philosophers could use the universal equations to fully model the mechanical universe.
As mathematical tools improved, increasingly more complex systems could be analysed, such as orbits of astrophysical objects or complex optical designs.
Furthermore, the development of computers allowed for an explosion in the size and breadth of tractable problems, such as dynamics of atomic nuclei, or weather forecast.

These applications were either developed from first principles, or as effective theories; but contained at their core, relatively simple mathematical models.
For example, modelling the dynamics of $n$-bodies gravitationally interacting are very rich and complicated to solve, but they all spawn from two simple equations: $\vec{F} = m \cdot \vec{a}$  and $F_g=G\frac{m_1  m_2}{r^2}$.

But the advent of computers not only gave us mathematical muscle, it provided us with large amounts of data on the real world.
Data that can be used to fit statistical models, even in the absence of underlying mathematical theories, such as recognising objects in images, or translating natural language.
The Scientific Revolution of the 16th century brought the concept \emph{``if it can be measured, it can be modelled"}, but the Data Revolution of the 20th century expanded it \emph{``if it can be \emph{represented}, it can be modelled"}.

Machine learning \marginpar{What is machine learning?}
is the study of statistical models that can create inferences from collections of examples. 
The models can be as concrete as relating the voltage and measured current intensity in a circuit, as complex as relating the sensory input of a rocket with its control, or as abstract as mapping natural images to the text describing its contents.



In general, designing a computer program fully capable of solving these problems can be time-consuming, and with a high degree of complexity.
Machine learning solves this limitation with data: instead of a programmer deciding every step, the algorithm depends on a series of free parameters that are deduced from the data.

\section{Classification and typology}
Machine learning tasks can be classified according to several criteria.
Here are, in broad strokes, some of the main types that cover the majority of the machine learning problems according to different criteria.

\subsection{Do we have labels?}
\begin{itemize}
\item \emph{Supervised:} our training data has assigned labels, and we want to predict them to new data. \emph{Ex: image recognition, linear regression.}
\item \emph{Unsupervised:} we do not have data with annotated target values. \emph{Ex: clustering, dimensionality reduction.}
\end{itemize}

The focus of this thesis will be on supervised tasks.

\subsection{Are labels categorical or continuous?}
The supervised tasks can be again divided depending on the nature of the labels:
\begin{itemize}
\item \emph{Classification:} our labels are categorical variables. \emph{Ex: image recognition, automated transcription of speech, presence or absence of tumours}
\item \emph{Regression:} we are interested in the value of continuous variables. \emph{Ex: curve fitting, counting.}
\end{itemize}

\section{The machine learning spectrum}
We can design machine learning models with different degrees of restrictions, or parametric assumptions.
A more restricted model needs less data to converge, and its performance will not be hindered if the underlying assumptions are correct.
On the other hand, if these restrictions are not accurate, the model will be biased and its performance, limited.

If we instead relax the parametric assumptions we obtain a more flexible model, capable of tackling more complex problems.
But this versatility comes with a cost: they require more data to train.

\begin{center}
	\missingfigure[figcolor=white]{Spectrum diagram}
\end{center}


Can we take it to the extreme?
\marginpar{A theoretical result} 
Can we train a model completely free  of assumptions in the case of infinite data? The No Free Lunch Theorem \citep{no_free_lunch} says, averaging over all problems, all algorithms are equally good.
In other words, without inputting domain knowledge, we cannot do better than random.

\section{A point of comparison: traditional machine learning}
In this section, I will give an overview to some of the most popular supervised machine learning algorithms to illustrate how they work, and the kind of underlying assumptions they operate under.

\subsection{Ordinary Least Squares, Ridge, and LASSO}
\subsection{Suport Vector Machines}
\subsection{Decision tree}
\subsection{Random Forest}
\subsection{Gaussian Processes}
\subsection{Monotonic regression}

\section{On the wrongness of machine learning}
